# Neural Network Implementation Overview

This project implements a basic 2-layer neural network from scratch using Python and NumPy. The network's goal is to learn patterns from the input data and make predictions that match the target output as closely as possible. The learning process involves forward propagation to compute outputs, backpropagation to update weights, and repeated iterations to minimize the loss.

## Neural Network Architecture

The neural network consists of:
1. **Input Layer**: Receives the input features. Each feature represents one dimension of the input data.
2. **Hidden Layer**: Composed of 4 neurons. This layer introduces non-linearity using an activation function (sigmoid), enabling the network to learn complex relationships.
3. **Output Layer**: A single neuron that generates the final prediction. The output value is a result of the weighted sum of the hidden layer outputs, passed through the sigmoid activation function.

## Key Concepts

### 1. Forward Propagation
   - Forward propagation is the process of passing input data through the network to generate an output.
   - **Steps Involved**:
     - The input data is multiplied by the weights associated with the connections to the hidden layer.
     - The result is passed through the sigmoid activation function, producing the hidden layer output.
     - The hidden layer output is then multiplied by the weights connected to the output neuron.
     - The final output is generated by applying the sigmoid activation function again.

### 2. Activation Function (Sigmoid)
   - The sigmoid function is used as the activation function for both the hidden and output layers. It squashes the input values into a range between 0 and 1.
   - This is especially useful for binary classification problems, where the output can be interpreted as a probability.
   - The function is defined as:
     \[
     \text{Sigmoid}(x) = \frac{1}{1 + e^{-x}}
     \]
   - It also provides a smooth gradient, which is helpful for the backpropagation process.

### 3. Loss Function (Sum-of-Squares)
   - The sum-of-squares loss function measures the difference between the predicted output and the actual target output.
   - The goal is to minimize this loss during training, which indicates that the network's predictions are becoming closer to the actual target values.
   - The loss is calculated as:
     \[
     \text{Loss} = \sum (\text{predicted} - \text{target})^2
     \]

### 4. Backpropagation
   - Backpropagation is used to calculate the gradients of the loss function with respect to the network's weights.
   - It involves propagating the error backward from the output layer to the hidden layer.
   - The gradients are used to update the weights in a way that minimizes the loss.
   - The update rules are based on the derivatives of the activation function (sigmoid) and the computed loss.

### 5. Weight Update
   - After calculating the gradients through backpropagation, the weights are updated by moving in the direction that reduces the loss.
   - The magnitude of the update is proportional to the gradient, ensuring that the weights gradually adjust to fit the data better.

## Training Process

The training involves multiple iterations (epochs) of forward propagation, loss calculation, backpropagation, and weight updates:
1. **Forward Propagation**: Computes the predicted output using the current weights.
2. **Loss Calculation**: Measures how far the predicted output is from the actual target.
3. **Backpropagation**: Calculates the gradients of the loss with respect to each weight.
4. **Weight Update**: Adjusts the weights based on the calculated gradients to reduce the loss.

This cycle is repeated for a specified number of epochs, gradually improving the network's predictions.

## Summary

The implemented neural network is a simple example of supervised learning, where the network learns to map input data to desired output values. By using techniques like forward propagation, sigmoid activation, sum-of-squares loss, and backpropagation, the network iteratively adjusts its weights to minimize prediction error. This foundational approach underlies more complex neural network architectures used in deep learning.
